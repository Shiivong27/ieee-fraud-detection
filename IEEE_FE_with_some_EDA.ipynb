{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime, math\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "MAKE_MODEL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':80000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------\n",
    "\n",
    "def make_test_predictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n",
    "    \n",
    "    new_columns = set(list(train_df)).difference(base_columns + remove_features)\n",
    "    features_columns = base_columns + list(new_columns)\n",
    "    \n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    for col in list(X):\n",
    "        if X[col].dtype=='O':\n",
    "            X[col] = X[col].fillna('unseen_before_label')\n",
    "            P[col] = P[col].fillna('unseen_before_label')\n",
    "\n",
    "            X[col] = train_df[col].astype(str)\n",
    "            P[col] = test_df[col].astype(str)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(list(X[col])+list(P[col]))\n",
    "            X[col] = le.transform(X[col])\n",
    "            P[col]  = le.transform(P[col])\n",
    "\n",
    "            X[col] = X[col].astype('category')\n",
    "            P[col] = P[col].astype('category')\n",
    "        \n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "\n",
    "    tr_data = lgb.Dataset(X, label=y)\n",
    "    vl_data = lgb.Dataset(P, label=P_y) \n",
    "    estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "    pp_p = estimator.predict(P)\n",
    "    predictions += pp_p/NFOLDS\n",
    "\n",
    "    if LOCAL_TEST:\n",
    "        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "        print(feature_imp)\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n",
    "    for col in columns:\n",
    "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
    "        if self_encoding:\n",
    "            train_df[col] = train_df[col].map(fq_encode)\n",
    "            test_df[col]  = test_df[col].map(fq_encode)            \n",
    "        else:\n",
    "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
    "                                 with_proportions=True, only_proportions=False):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            if only_proportions:\n",
    "                train_df[new_col] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col]  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "            if with_proportions:\n",
    "                train_df[new_col+'_proportions'] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col+'_proportions']  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def uid_aggregation_and_normalization(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            \n",
    "            new_norm_col_name = col+'_'+main_column+'_std_norm'\n",
    "            norm_cols = []\n",
    "            \n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "                norm_cols.append(new_col_name)\n",
    "            \n",
    "            train_df[new_norm_col_name] = (train_df[main_column]-train_df[norm_cols[0]])/train_df[norm_cols[1]]\n",
    "            test_df[new_norm_col_name]  = (test_df[main_column]-test_df[norm_cols[0]])/test_df[norm_cols[1]]          \n",
    "            \n",
    "            del train_df[norm_cols[0]], train_df[norm_cols[1]]\n",
    "            del test_df[norm_cols[0]], test_df[norm_cols[1]]\n",
    "                                              \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cor_and_remove(train_df, test_df, i_cols, new_columns, remove=False):\n",
    "    # Check correllation\n",
    "    print('Correlations','#'*10)\n",
    "    for col in new_columns:\n",
    "        cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "        print(col, cor_cof)\n",
    "\n",
    "    if remove:\n",
    "        print('#'*10)\n",
    "        print('Best options:')\n",
    "        best_fe_columns = []\n",
    "        for main_col in i_cols:\n",
    "            best_option = ''\n",
    "            best_cof = 0\n",
    "            for col in new_columns:\n",
    "                if main_col in col:\n",
    "                    cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "                    cor_cof = (cor_cof**2)**0.5\n",
    "                    if cor_cof>best_cof:\n",
    "                        best_cof = cor_cof\n",
    "                        best_option = col\n",
    "\n",
    "            print(main_col, best_option, best_cof)            \n",
    "            best_fe_columns.append(best_option)\n",
    "\n",
    "        for col in new_columns:\n",
    "            if col not in best_fe_columns:\n",
    "                del train_df[col], test_df[col]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
